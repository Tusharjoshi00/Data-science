{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd310bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep learning\n",
    "### input layer---> hidden layer(s) ---> output layer\n",
    "### input layer: 1st layer, input data\n",
    "###input nodes depend on the number of features\n",
    "### hidden layer: 2nd layer, can have multiple hidden layers\n",
    "###output layer: last layer, output data\n",
    "### output nodes depend on the number of classes\n",
    "### 1. binary classification: in this we classify data in two class like ----0 or 1, y/n, win/lose, etc.\n",
    "### 2. multi-class classification: number of output nodes = number of classes\n",
    "##regression number of nodes are 1 because we are predicting a single value\n",
    "### activation function: it is a function that takes input and produces output, it is used to \n",
    "# introduce non-linearity in the model\n",
    "### activation function is applied to the output of each node in the hidden layer and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perceptron ----> single layer neural network or smallest unit or single unit of neural network\n",
    "# in this x = input \n",
    "# w = weight ----> it is trainable parameter, it is multiplied with input--> intialize randomly\n",
    "# b = bias ----> it is also trainable parameter, it is added to the output\n",
    "#  b = bias, y = output , z=weighted sum=submision WiXi\n",
    "# b= is a constant which add on z so that it can a value \n",
    "#activation functon ==> add non linearity  to solve complex problems\n",
    "#Types of activation function:\n",
    "# 1. sigmoid function: OL -- it is used for binary classification, it squashes the output \n",
    "# 2. tanh function: HL-- -1 TO 1 --- it is used for binary classification, it squashes the output between -1 and 1\n",
    "# 3. relu function: HL ---it is used for multi-class classification, it is used to avoid vanishing gradient problem, it is mostly in use\n",
    "# 4. softmax function: HL---it is used for multi-class classification, it is used to convert the output to probability distribution\n",
    "# 5. linear function: HL---- it is used for regression, it is used to predict a single value\n",
    "# 6. leaky relu function: HL ----it is used for multi-class classification, it is used to avoid vanishing gradient problem\n",
    "# 7. swish function: HL--- it is used for multi-class classification, it is used to avoid vanishing gradient problem\n",
    "# 8. gelu function: HL----it is used for multi-class classification, it is used to avoid vanishing gradient problem\n",
    "# 9. softmax function: OL----it is used for multi-class classification, it is used to convert the output to probability distribution\n",
    "# 10. Linear function: ---Reg task ---f(x)= x----it is used for regression, it is used to predict a single value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c008839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH : when a model complete one forward and backward cycle of all training samples\n",
    "# derivative range : "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
